{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54aafe7f",
   "metadata": {},
   "source": [
    "### 9-1. 프로젝트 : 모든 장르 간 편향성 측정해 보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0157146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.2\n",
      "4.1.2\n",
      "1.0\n",
      "0.11.2\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "import gensim\n",
    "import sklearn\n",
    "import seaborn\n",
    "\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(seaborn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c3c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사운드 엔지니어 상우(유지태 분)는 치매에 걸린 할머니(백성희 분)와\n",
      " 젊은 시절 상처한 한 아버지(박인환 분), 고모(신신애 분)와 함께 살고 있다.\n",
      " 어느 겨울 그는 지방 방송국 라디오 PD 은수(이영애 분)를 만난다.\n",
      " 자연의 소리를 채집해 틀어주는 라디오 프로그램을 준비하는 은수는 상우와 녹음 여행을 떠난다.\n",
      " 자연스레 가까워지는 두 사람은 어느 날, 은수의 아파트에서 밤을 보낸다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    for i in range(5):\n",
    "        print(file.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56278a",
   "metadata": {},
   "source": [
    "### STEP 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기 with synopsis.txt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b29b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True)\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" 등을 포함할 수도 있습니다.\n",
    "                res.append(w[0])   \n",
    "        tokenized.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "964ef45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71156\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf4ec6",
   "metadata": {},
   "source": [
    "### STEP 2. 추출된 결과로 embedding model 만들기  (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a41747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.8919191956520081),\n",
       " ('다큐멘터리', 0.8583681583404541),\n",
       " ('드라마', 0.8124423623085022),\n",
       " ('영화로', 0.8040297627449036),\n",
       " ('코미디', 0.7889005541801453),\n",
       " ('형식', 0.7784495949745178),\n",
       " ('주제', 0.7737573385238647),\n",
       " ('소재', 0.77344810962677),\n",
       " ('설정', 0.7732013463973999),\n",
       " ('스토리', 0.7731793522834778)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fd3bd",
   "metadata": {},
   "source": [
    "### STEP 3. target, attribute 단어 셋 만들기     \n",
    "이전 스텝에서는 TF-IDF를 사용해서 단어 셋을 만들었습니다. 이 방법으로도 어느 정도는 대표 단어를 잘 선정할 수 있습니다. 그러나 TF-IDF가 높은 단어를 골랐음에도 불구하고 중복되는 단어가 발생하는 문제가 있었습니다. 개념축을 표현하는 단어가 제대로 선정되지 않은 것은 WEAT 계산 결과에 악영향을 미칩니다.   \n",
    "\n",
    "TF-IDF를 적용했을 때의 문제점이 무엇인지 지적 가능하다면 그 문제점을 지적하고 스스로 방법을 개선하여 대표 단어 셋을 구축해 보기 바랍니다. TF-IDF 방식을 쓰더라도 중복된 단어를 잘 제거하면 여전히 유용한 방식이 될 수 있습니다\n",
    "\n",
    "> 기존 노드에서 진행하였던 방식(우선 상위 100개의 단어들 중 중복되는 단어를 제외)는 각 카테고리에 적합한 단어를 중복의 이유로 제거할 수 있다는 문제가 있다. 예를들어 '아버지'는 Family에 굉장히 적합한 단어로 사료되지만 드라마 같은 카테고리에도 중복되어 사용되므로 제거된다. 이러한 제거 방식은 각 카테고리의 적절한 대표 단어 선정을 저해할 가능성이 있다.\n",
    "\n",
    ">Solution) 공통 단어라도 특정 장르에서 더 중요한 단어는 유지 (중복 단어중, 특정 카테고리에서 TF-IDF 값이 높은 경우 그 카테고리에서 유지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de29405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "#              'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "#              'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "#              'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "#              'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "# genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "#          '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1a7501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간상 '가족'& '드라마'로 비교\n",
    "genre_txt = [ 'synopsis_family.txt', 'synopsis_drama.txt']\n",
    "genre_name = ['가족',  '드라마']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e832eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c65495a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_family.txt 파일을 읽고 있습니다.\n",
      "synopsis_drama.txt 파일을 읽고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f50700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24276)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre) # 11개( SF & Family & show ... & drama)를 숫자로 변환\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e4fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22154)\t0.00012912989604844745\n",
      "  (0, 21084)\t0.00012912989604844745\n",
      "  (0, 12586)\t0.0002582597920968949\n",
      "  (0, 21085)\t0.00038738968814534233\n",
      "  (0, 17913)\t0.0005165195841937898\n",
      "  (0, 6810)\t0.00012912989604844745\n",
      "  (0, 11142)\t0.0002582597920968949\n",
      "  (0, 20458)\t0.0005165195841937898\n",
      "  (0, 7707)\t0.0002582597920968949\n",
      "  (0, 12522)\t0.00012912989604844745\n",
      "  (0, 16334)\t0.00012912989604844745\n",
      "  (0, 13127)\t0.00012912989604844745\n",
      "  (0, 20761)\t0.00012912989604844745\n",
      "  (0, 21588)\t0.0002582597920968949\n",
      "  (0, 9399)\t0.00012912989604844745\n",
      "  (0, 20104)\t0.00012912989604844745\n",
      "  (0, 2751)\t0.00012912989604844745\n",
      "  (0, 4364)\t0.00012912989604844745\n",
      "  (0, 19269)\t0.00012912989604844745\n",
      "  (0, 2870)\t0.00012912989604844745\n",
      "  (0, 14564)\t0.00012912989604844745\n",
      "  (0, 19405)\t0.00012912989604844745\n",
      "  (0, 22466)\t0.00012912989604844745\n",
      "  (0, 12948)\t0.00012912989604844745\n",
      "  (0, 13008)\t0.00038738968814534233\n",
      "  :\t:\n",
      "  (0, 9571)\t0.20920389386122448\n",
      "  (0, 10742)\t0.0446522145000242\n",
      "  (0, 15748)\t0.0045019722438296005\n",
      "  (0, 10036)\t0.011852131009265682\n",
      "  (0, 16993)\t0.005053234151237307\n",
      "  (0, 13499)\t9.187698456795103e-05\n",
      "  (0, 12255)\t0.006339511935188621\n",
      "  (0, 2309)\t0.00045938492283975515\n",
      "  (0, 14984)\t0.0033075714444462372\n",
      "  (0, 12378)\t0.008452682580251494\n",
      "  (0, 13623)\t0.11071176640438099\n",
      "  (0, 9057)\t0.005145111135805258\n",
      "  (0, 18856)\t0.003031940490742384\n",
      "  (0, 3971)\t0.021774845342604394\n",
      "  (0, 13278)\t0.0007350158765436082\n",
      "  (0, 15289)\t0.028114357277793014\n",
      "  (0, 8093)\t9.187698456795103e-05\n",
      "  (0, 14928)\t0.00045938492283975515\n",
      "  (0, 16795)\t0.30521534273473333\n",
      "  (0, 16027)\t0.025725555679026287\n",
      "  (0, 18329)\t0.01809976595988635\n",
      "  (0, 7458)\t0.023612385033963415\n",
      "  (0, 133)\t0.04272279782409723\n",
      "  (0, 16994)\t0.0016537857222231186\n",
      "  (0, 6519)\t0.06835647651855557\n"
     ]
    }
   ],
   "source": [
    "print(X[1]) # doc Idx, word idx, TF-IDF score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c4e7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family 대표하는 단어들:\n",
      "엄마, 영화제, 아빠, 가족, 자신, 위해, 친구, 아버지, 시작, 그녀, 아들, 마을, 국제, 사람, 아이, 사랑, 할머니, 학교, 세상, 소년, 이야기, 가장, 어머니, 아주르, 소녀, 대한, 모두, 아내, 사건, 사이, 마음, 혼자, 낙타, 서울, 모험, 위기, 다시, 과연, 결심, 씨제이, 엠마, 할아버지, 서로, 미아, 하나, 작품, 영화, 발견, 한편, 동구, 이자, 고양이, 기억, 도시, 시골, 크리스마스, 단편, 다른, 생각, 시간, 소식, 때문, 간다, 부산, 사실, 손녀, 통해, 요정, 인형, 도움, 인간, 상황, 모든, 해나, 케이시, 편지, 인생, 테리, 순간, 매일, 스튜어트, 여행, 번개, 동생, 아기, 회사, 펠리칸, 슈이트, 점점, 선물, 부부, 가지, 인도, 마르, 아스, 부모님, 동안, 모습, 판타스틱, 부천, \n",
      "\n",
      "Drama 대표하는 단어들:\n",
      "자신, 영화제, 그녀, 사람, 사랑, 위해, 영화, 시작, 국제, 남자, 친구, 이야기, 아버지, 엄마, 가족, 여자, 대한, 아들, 아이, 마음, 단편, 서울, 남편, 서로, 시간, 소녀, 다른, 세상, 감독, 모든, 연출, 다시, 아내, 마을, 사이, 사건, 관계, 생각, 작품, 사실, 모습, 통해, 소년, 때문, 하나, 어머니, 학교, 생활, 간다, 점점, 결혼, 우리, 발견, 대해, 인생, 순간, 여행, 여성, 죽음, 부문, 기억, 상황, 세계, 현실, 가장, 일상, 부산, 독립, 감정, 마지막, 과연, 모두, 과거, 의도, 가지, 한편, 경쟁, 하루, 음악, 배우, 이제, 할머니, 연인, 한국, 아빠, 결심, 동안, 이자, 미국, 전쟁, 사고, 주인공, 동생, 처음, 시절, 도시, 사회, 비밀, 명의, 상처, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('Family 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('Drama 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca579e",
   "metadata": {},
   "source": [
    "### 중복단어 모두 제거시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e898faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "108a15e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아주르', '혼자', '낙타', '모험', '위기', '씨제이', '엠마', '할아버지', '미아', '동구', '고양이', '시골', '크리스마스', '소식', '손녀']\n",
      "['남자', '여자', '남편', '감독', '연출', '관계', '생활', '결혼', '우리', '대해', '여성', '죽음', '부문', '세계', '현실']\n"
     ]
    }
   ],
   "source": [
    "print(target_art)\n",
    "print(target_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5491eb1",
   "metadata": {},
   "source": [
    "### IDF Score가 높은 곳에 남길시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84a7b3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_list = []  \n",
    "length = len(genre_txt)\n",
    "for i in range(length):\n",
    "    m_list.append(X[i].tocoo())\n",
    "len(m_list)\n",
    "\n",
    "w_list = []  \n",
    "for i in range(length):\n",
    "    w_list.append([[i, j] for i, j in zip(m_list[i].col, m_list[i].data)])\n",
    "len(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c2fa36bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2470"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "069bc294",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w1_, new_w2_ = {}, {}\n",
    "\n",
    "for i in range(100):\n",
    "    new_w1_[vectorizer.get_feature_names_out()[w1[i][0]]] = w1[i][1]  # word and its weight for w1\n",
    "    new_w2_[vectorizer.get_feature_names_out()[w2[i][0]]] = w2[i][1]  # word and its weight for w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "124c3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01  # TF-IDF 점수 차이가 0.01 이상일 경우\n",
    "                  # 중복을 어느정도 허용하고 싶으면 threshold 증가\n",
    "remove_from_art = set()\n",
    "remove_from_gen = set()\n",
    "\n",
    "duplicated_words = set(new_w1_.keys()) & set(new_w2_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb9414e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family, Drama 간 중복되는 단어들\n",
      "{'모습', '친구', '가지', '가족', '사실', '다시', '간다', '도시', '결심', '부산', '동생', '다른', '영화제', '점점', '단편', '발견', '마을', '어머니', '소년', '인생', '마음', '여행', '학교', '기억', '사랑', '아빠', '순간', '사건', '아들', '한편', '사이', '위해', '하나', '아내', '동안', '국제', '가장', '때문', '모든', '이자', '서로', '작품', '시간', '그녀', '이야기', '아버지', '생각', '엄마', '시작', '자신', '영화', '서울', '할머니', '과연', '소녀', '상황', '모두', '아이', '대한', '사람', '통해', '세상'}\n"
     ]
    }
   ],
   "source": [
    "print('Family, Drama 간 중복되는 단어들')\n",
    "print(duplicated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c863abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family 제거 단어: {'모습', '사실', '다른', '영화제', '점점', '단편', '마음', '사랑', '국제', '때문', '모든', '서로', '시간', '그녀', '이야기', '생각', '시작', '자신', '영화', '서울', '대한', '사람', '통해'}\n",
      "====================\n",
      "Drama 제거 단어: {'친구', '가족', '도시', '결심', '마을', '어머니', '소년', '학교', '아빠', '한편', '아들', '위해', '가장', '이자', '아버지', '엄마', '할머니', '과연', '모두', '아이', '세상'}\n"
     ]
    }
   ],
   "source": [
    "# 중복시 TF-IDF 계산하여 제거 단어 선정\n",
    "for w in common_words:\n",
    "    tfidf_art = new_w1_.get(w, 0)\n",
    "    tfidf_gen = new_w2_.get(w, 0)\n",
    "\n",
    "    if tfidf_art > tfidf_gen + threshold:\n",
    "        remove_from_gen.add(w)  \n",
    "    elif tfidf_gen > tfidf_art + threshold:\n",
    "        remove_from_art.add(w) \n",
    "\n",
    "# 결과 출력\n",
    "print(\"Family 제거 단어:\", remove_from_art)\n",
    "print('====================')\n",
    "print(\"Drama 제거 단어:\", remove_from_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6becaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family 상위 15개중에서 중복 단어 수\n",
      "영화제\n",
      "자신\n",
      "시작\n",
      "그녀\n",
      "국제\n",
      "사람\n",
      "6 개\n"
     ]
    }
   ],
   "source": [
    "# Family 상위 15개중 제거 단어에 포함된 갯수\n",
    "w1.sort(key=lambda x: x[1], reverse=True)\n",
    "cnt = 0\n",
    "print('Family 상위 15개중에서 제거 단어 수')\n",
    "for i in range(15):\n",
    "    temp = vectorizer.get_feature_names()[w1[i][0]]\n",
    "    if temp in remove_from_art:\n",
    "        print(temp)\n",
    "        cnt += 1\n",
    "print(cnt,'개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ee23806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family 상위 15개중에서 제거 단어 수\n",
      "위해\n",
      "친구\n",
      "아버지\n",
      "엄마\n",
      "가족\n",
      "5 개\n"
     ]
    }
   ],
   "source": [
    "# Drama 상위 15개중 제거 단어에 포함된 갯수\n",
    "w2.sort(key=lambda x: x[1], reverse=True)\n",
    "cnt = 0\n",
    "print('Family 상위 15개중에서 제거 단어 수')\n",
    "for i in range(15):\n",
    "    temp = vectorizer.get_feature_names()[w2[i][0]]\n",
    "    if temp in remove_from_gen:\n",
    "        print(temp)\n",
    "        cnt += 1\n",
    "print(cnt,'개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96ea85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거\n",
    "filtered_w1 = [w for w in w1 if vectorizer.get_feature_names_out()[w[0]] not in remove_from_art]\n",
    "filtered_w2 = [w for w in w2 if vectorizer.get_feature_names_out()[w[0]] not in remove_from_gen]\n",
    "\n",
    "# TF-IDF 점수대로 정렬\n",
    "filtered_w1.sort(key=lambda x: x[1], reverse=True)\n",
    "filtered_w2.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4f9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 리스트 변환\n",
    "filtered_family_words = [vectorizer.get_feature_names_out()[w[0]] for w in filtered_w1[:100]]\n",
    "filtered_drama_words = [vectorizer.get_feature_names_out()[w[0]] for w in filtered_w2[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e757abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family 단어 15개: ['엄마', '아빠', '가족', '위해', '친구', '아버지', '아들', '마을', '아이', '할머니', '학교', '세상', '소년', '가장', '어머니']\n",
      "Drama 단어 15개: ['자신', '영화제', '그녀', '사람', '사랑', '영화', '시작', '국제', '남자', '이야기', '여자', '대한', '마음', '단편', '서울']\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "target_family, target_drama = [], []\n",
    "\n",
    "for word in filtered_family_words:\n",
    "    if word in model.wv:  # Word2Vec에 있는 단어만 사용\n",
    "        target_family.append(word)\n",
    "    if len(target_family) == n:\n",
    "        break\n",
    "\n",
    "for word in filtered_drama_words:\n",
    "    if word in model.wv:\n",
    "        target_drama.append(word)\n",
    "    if len(target_drama) == n:\n",
    "        break\n",
    "\n",
    "print(\"Family 단어 15개:\", target_family)\n",
    "print(\"Drama 단어 15개:\", target_drama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c4b4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이전과 비교 ###\n",
    "# ['아주르', '혼자', '낙타', '모험', '위기', '씨제이', '엠마', '할아버지', '미아', '동구', '고양이', '시골', '크리스마스', '소식', '손녀']\n",
    "# ['남자', '여자', '남편', '감독', '연출', '관계', '생활', '결혼', '우리', '대해', '여성', '죽음', '부문', '세계', '현실']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26201f59",
   "metadata": {},
   "source": [
    "공통적으로 중요한 경우도 있으며, TF-IDF 점수가 높다고 해서 특정한 카테고리를 대표하는 단어라고 보장할 수 없다. 하지만 빈도기반인 TF-IDF에서 해당 방법이 단순 제거보다 효율적으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef94d0",
   "metadata": {},
   "source": [
    "### 4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b31a7a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24276)\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(genre) # Family & Drama\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d2f1bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가족', '드라마']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3916445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가족: 엄마, 영화제, 아빠, 가족, 자신, 위해, 친구, 아버지, 시작, 그녀, 아들, 마을, 국제, 사람, 아이, \n",
      "드라마: 자신, 영화제, 그녀, 사람, 사랑, 위해, 영화, 시작, 국제, 남자, 친구, 이야기, 아버지, 엄마, 가족, \n"
     ]
    }
   ],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d390ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af44b6e",
   "metadata": {},
   "source": [
    "### WEAT 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e5ba9",
   "metadata": {},
   "source": [
    "#### 1. 중복 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4342c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아주르', '혼자', '낙타', '모험', '위기', '씨제이', '엠마', '할아버지', '미아', '동구', '고양이', '시골', '크리스마스', '소식', '손녀']\n",
      "['남자', '여자', '남편', '감독', '연출', '관계', '생활', '결혼', '우리', '대해', '여성', '죽음', '부문', '세계', '현실']\n"
     ]
    }
   ],
   "source": [
    "print(target_art)\n",
    "print(target_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e756f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6105e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가족 드라마 1.4275923\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf10d4b",
   "metadata": {},
   "source": [
    "#### 2. 중복 단어를 가장 score가 높은곳에 할당할 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f6d71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['엄마', '아빠', '가족', '위해', '친구', '아버지', '아들', '마을', '아이', '할머니', '학교', '세상', '소년', '가장', '어머니']\n",
      "['자신', '영화제', '그녀', '사람', '사랑', '영화', '시작', '국제', '남자', '이야기', '여자', '대한', '마음', '단편', '서울']\n"
     ]
    }
   ],
   "source": [
    "print(target_family)\n",
    "print(target_drama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4df1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[word] for word in target_family])\n",
    "Y = np.array([model.wv[word] for word in target_drama])\n",
    "\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7db351f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가족 드라마 1.6726209\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea91c2",
   "metadata": {},
   "source": [
    "주요 단어 선정에 있어 단순 중복 제거보다 조금 개선된 부분이 있는 것처럼 보이지만, TF-IDF 방식 사용시 더 정교한 처리가 필요해 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
