{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878bc36f",
   "metadata": {},
   "source": [
    "### 24-1. 커스텀 프로젝트 직접 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6140dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import create_optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import time\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, TFBertForSequenceClassification, DataCollatorWithPadding\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c421d",
   "metadata": {},
   "source": [
    "#### STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1336feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4be28c91d44c038c1c32d2a6c32694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('nsmc')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60769e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# STEP 2. klue/bert-base model 및 tokenizer 불러오기\n",
    "\n",
    "\n",
    "# TF model\n",
    "# model_name = \"klue/bert-base\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2, from_pt=True) \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b447b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9976970', 'label': 0, 'document': '아 더빙.. 진짜 짜증나네요 목소리'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf361ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5703dd291b8942fdb46f4ab5e87e914f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5f90aec3af4326a5b3d962efacf036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff0670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np array 변환 \n",
    "train_inputs = {key: np.array(tokenized_datasets[\"train\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
    "train_labels = np.array(tokenized_datasets[\"train\"][\"label\"])\n",
    "\n",
    "test_inputs = {key: np.array(tokenized_datasets[\"test\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
    "test_labels = np.array(tokenized_datasets[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91091a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   2, 1376,  831, ...,    0,    0,    0],\n",
       "        [   2, 1963,   18, ...,    0,    0,    0],\n",
       "        [   2,    1,    3, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 4380, 1097, ...,    0,    0,    0],\n",
       "        [   2, 9300, 3771, ...,    0,    0,    0],\n",
       "        [   2, 3629, 3771, ...,    0,    0,    0]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd270aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 128)\n",
      "(150000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs['input_ids'].shape)\n",
    "print(train_inputs['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffdf06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d7278ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset으로 변환\n",
    "train_data = Dataset.from_dict(train_inputs)\n",
    "test_data = Dataset.from_dict(test_inputs)\n",
    "\n",
    "train_data = train_data.add_column(\"labels\", train_labels)\n",
    "test_data = test_data.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0488318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ae890f3",
   "metadata": {},
   "source": [
    "#### [Pytorch] baseline 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2f6b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy  metric필요\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  \n",
    "    predictions = np.argmax(logits, axis=-1)  \n",
    "    return metric.compute(predictions=predictions, references=labels)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c88344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def inference(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()  # .cpu()로 CPU로 이동\n",
    "    pred_label = np.argmax(probabilities)\n",
    "\n",
    "    result = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    print(f\"{sentence} , {result} ({probabilities[0][pred_label]:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93260f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./assets',              \n",
    "    num_train_epochs=2,                  \n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=8,        \n",
    "    logging_dir='./logs',                \n",
    "    logging_steps=1,                     \n",
    "    evaluation_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",               \n",
    "    report_to=\"none\",                    \n",
    "    logging_first_step=True,             \n",
    "    load_best_model_at_end=True,        \n",
    "    metric_for_best_model=\"accuracy\",  \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,         \n",
    "    compute_metrics=compute_metrics      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e26eab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 1.71 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 2:33:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.311077</td>\n",
       "      <td>0.883540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.394901</td>\n",
       "      <td>0.892460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-18750\n",
      "Configuration saved in ./results/checkpoint-18750/config.json\n",
      "Model weights saved in ./results/checkpoint-18750/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-37500\n",
      "Configuration saved in ./results/checkpoint-37500/config.json\n",
      "Model weights saved in ./results/checkpoint-37500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-37500 (score: 0.89246).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 2.53 GB\n",
      "Training Time: 9253.83 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93926b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화 그냥 그래.. 볼지 말지는 너 선택이야. , Negative (0.9862%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"이 영화 그냥 그래.. 볼지 말지는 너 선택이야.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cf4139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화 완전 별로인것 같았는데 꿀잼이야. , Positive (0.9859%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"이 영화 완전 별로인것 같았는데 꿀잼이야.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75c207a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 왜 영화관에 사람들이 없는지 알겠다. , Negative (0.9475%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"아 왜 영화관에 사람들이 없는지 알겠다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f6f6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 재미있긴 해. , Positive (0.9747%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"근데 재미있긴 해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28f87656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재미있어서 상영내내 꿀잠잤어. , Positive (0.9582%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99d044da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너~무 재미있어서 상영내내 꿀잠잤어. , Positive (0.9656%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"너~무 재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26bf7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너무 재미있어서 상영내내 꿀잠잤어. , Positive (0.9885%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"너무 재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7509e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38984cfaffbe4e31abbd6791c82aba10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07d70d10bca4ac9be130acdb18cbd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Bucketing test ######\n",
    "\n",
    "def tokenize_function_new(examples):\n",
    "    return tokenizer(examples[\"document\"], truncation=True)  \n",
    "\n",
    "tokenized_datasets_b = dataset.map(tokenize_function_new, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0ccb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "    num_rows: 150000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_b['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32dd9f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59/1690426973.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_inputs = {key: np.array(tokenized_datasets_b[\"train\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
      "/tmp/ipykernel_59/1690426973.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_inputs = {key: np.array(tokenized_datasets_b[\"test\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n"
     ]
    }
   ],
   "source": [
    "# NumPy 배열로 변환\n",
    "train_inputs = {key: np.array(tokenized_datasets_b[\"train\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
    "train_labels = np.array(tokenized_datasets_b[\"train\"][\"label\"])\n",
    "\n",
    "test_inputs = {key: np.array(tokenized_datasets_b[\"test\"][key]) for key in [\"input_ids\", \"attention_mask\"]}\n",
    "test_labels = np.array(tokenized_datasets_b[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db86374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({**train_inputs, \"labels\": train_labels})\n",
    "test_data = Dataset.from_dict({**test_inputs, \"labels\": test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9597ef3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   2, 1376,  831, ...,    0,    0,    0],\n",
       "        [   2, 1963,   18, ...,    0,    0,    0],\n",
       "        [   2,    1,    3, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 4380, 1097, ...,    0,    0,    0],\n",
       "        [   2, 9300, 3771, ...,    0,    0,    0],\n",
       "        [   2, 3629, 3771, ...,    0,    0,    0]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0dea80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  \n",
    "    predictions = np.argmax(logits, axis=-1)  \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f695a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 29:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>0.340221</td>\n",
       "      <td>0.892920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./bucket/checkpoint-18750\n",
      "Configuration saved in ./bucket/checkpoint-18750/config.json\n",
      "Model weights saved in ./bucket/checkpoint-18750/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Memory: 3.40 GB, Time: 1804.12 sec\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "training_args1 = TrainingArguments(\n",
    "    output_dir='./assets',              \n",
    "    num_train_epochs=1,                  \n",
    "    per_device_train_batch_size=8,       \n",
    "    evaluation_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",               \n",
    "    group_by_length=True,  # Bucketing \n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,         \n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer1.train()\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d058c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화 그냥 그래.. 볼지 말지는 너 선택이야. , Negative (0.9871%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"이 영화 그냥 그래.. 볼지 말지는 너 선택이야.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9651efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화 완전 별로인것 같았는데 꿀잼이야. , Positive (0.9925%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"이 영화 완전 별로인것 같았는데 꿀잼이야.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03fd4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재미있어서 상영내내 꿀잠잤어. , Negative (0.9125%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55a5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너~무 재미있어서 상영내내 꿀잠잤어. , Negative (0.8062%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"너~무 재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de366095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 38:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.339782</td>\n",
       "      <td>0.887200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./bucket/checkpoint-18750\n",
      "Configuration saved in ./bucket/checkpoint-18750/config.json\n",
      "Model weights saved in ./bucket/checkpoint-18750/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Memory: 3.47 GB, Time: 2328.61 sec\n"
     ]
    }
   ],
   "source": [
    "#Default learning rate: 5e-5 / AdamW\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir='./assets',              \n",
    "    num_train_epochs=1,                  \n",
    "    per_device_train_batch_size=8,       \n",
    "    evaluation_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",               \n",
    "    group_by_length=False,  \n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,         \n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "start_time = time.time()\n",
    "trainer2.train()\n",
    "end_time = time.time()\n",
    "max_mem_no_bucket = torch.cuda.max_memory_allocated() / 1024 ** 3  \n",
    "print(f\"Max Memory: {max_mem_no_bucket:.2f} GB, Time: {end_time - start_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec85fb",
   "metadata": {},
   "source": [
    "### Pytorch Base line\n",
    "- Each epoch takes around an hour \n",
    "- group_by_length=True (Bucketing)\t: 배치 내 샘플 길이를 비슷하게 묶음\n",
    "- DataCollatorWithPadding : 배치 내에서 패딩을 자동으로 맞춤   \n",
    "- [ 위 결과 ]\n",
    "즉 문장토큰 길이를 파악하지 않고 진행하는 경우(max_length=128) 대략 4500sec 시간이 걸렸으나, DataCollatorWithPadding 만 사용했을 경우 2328.61 sec, Bucketing & DataCollatorWithPadding 같이 사용한 경우 1804.12 sec로 학습 속도를 줄일 수 있었다. 이는 패딩에 들어가는 메모리를 줄임으로써 최적화로부터 나온 차이로 사료된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf1114",
   "metadata": {},
   "source": [
    "#### [TF] baseline 2 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c5ac97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Loading PyTorch weights from /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "PyTorch checkpoint contains 135,851,522 parameters\n",
      "Loaded 110,617,344 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9375/9375 [==============================] - 4357s 464ms/step - loss: 0.2719 - accuracy: 0.8851 - val_loss: 0.2443 - val_accuracy: 0.8985\n",
      "Epoch 2/2\n",
      "9375/9375 [==============================] - 4347s 464ms/step - loss: 0.1822 - accuracy: 0.9282 - val_loss: 0.2669 - val_accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cbc063d6f40>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# PyTorch to TensorFlow 변환\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2, from_pt=True)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-5) \n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "model.fit(train_inputs, train_labels, validation_data=(test_inputs, test_labels), batch_size=16, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "763e37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    logits = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]).logits\n",
    "    \n",
    "    probabilities = tf.nn.softmax(logits) \n",
    "    pred_label = np.argmax(probabilities)  \n",
    "\n",
    "    result = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    print(f\" {sentence} , {result} ({probabilities.numpy()[0][pred_label]:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03ece239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 재미있어서 상영내내 꿀잠잤어. , Negative (0.6970%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea1523d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 너~무 재미있어서 상영내내 꿀잠잤어. , Negative (0.6475%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"너~무 재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8b5e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 너무 재미있어서 상영내내 꿀잠잤어. , Positive (0.5792%)\n"
     ]
    }
   ],
   "source": [
    "inference(\"너무 재미있어서 상영내내 꿀잠잤어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0621dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  110617344 \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 110,618,882\n",
      "Trainable params: 110,618,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33500577",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4dec5",
   "metadata": {},
   "source": [
    "### 전체 학습 3 epoch tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9d915",
   "metadata": {},
   "source": [
    "### Best Val Acc : 0.9022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81d6c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "9375/9375 [==============================] - 4185s 445ms/step - loss: 0.2728 - accuracy: 0.8854 - val_loss: 0.2374 - val_accuracy: 0.9033\n",
      "Epoch 2/3\n",
      "9375/9375 [==============================] - 4172s 445ms/step - loss: 0.1832 - accuracy: 0.9277 - val_loss: 0.2619 - val_accuracy: 0.9001\n",
      "Epoch 3/3\n",
      "9375/9375 [==============================] - 4173s 445ms/step - loss: 0.1207 - accuracy: 0.9548 - val_loss: 0.2728 - val_accuracy: 0.9022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x77fb841a3250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-5)  # Trainer Default : 5e-5\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "model.fit(train_inputs, train_labels, validation_data=(test_inputs, test_labels), batch_size=16, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8275030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inference(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    logits = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]).logits\n",
    "    \n",
    "    probabilities = tf.nn.softmax(logits) \n",
    "    pred_label = np.argmax(probabilities)  \n",
    "\n",
    "    result = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    print(f\" {sentence} , {result} ({probabilities.numpy()[0][pred_label]:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6db7bb",
   "metadata": {},
   "source": [
    "### Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40b9fc",
   "metadata": {},
   "source": [
    "#### 9 ~ 12 Trarin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8534644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 52\n"
     ]
    }
   ],
   "source": [
    "model.bert.embeddings.trainable = False  \n",
    "\n",
    "for i in range(9):  \n",
    "    model.bert.encoder.layer[i].trainable = False  \n",
    "\n",
    "# 3 layers\n",
    "for i in range(9, 12):  \n",
    "    model.bert.encoder.layer[i].trainable = True  \n",
    "\n",
    "trainable_count = sum([tf.reduce_sum(tf.cast(v.trainable, tf.int32)).numpy() for v in model.trainable_variables])\n",
    "print(f\"Trainable layers: {trainable_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d27f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4688/4688 [==============================] - 2362s 501ms/step - loss: 0.3210 - accuracy: 0.8601 - val_loss: 0.2815 - val_accuracy: 0.8813\n",
      "Epoch 2/2\n",
      "4688/4688 [==============================] - 2344s 500ms/step - loss: 0.2680 - accuracy: 0.8865 - val_loss: 0.2610 - val_accuracy: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x77fa1fad01f0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-5)  # Fine-tuning 시에는 작은 학습률이 좋음\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_inputs, train_labels, validation_data=(test_inputs, test_labels), batch_size=32, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb09def",
   "metadata": {},
   "source": [
    "#### 6 Trarin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a36eddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 100\n"
     ]
    }
   ],
   "source": [
    "model.bert.embeddings.trainable = False  \n",
    "\n",
    "for i in range(6):  \n",
    "    model.bert.encoder.layer[i].trainable = False  \n",
    "\n",
    "# 절반 train\n",
    "for i in range(6, 12):  \n",
    "    model.bert.encoder.layer[i].trainable = True  \n",
    "\n",
    "trainable_count = sum([tf.reduce_sum(tf.cast(v.trainable, tf.int32)).numpy() for v in model.trainable_variables])\n",
    "print(f\"Trainable layers: {trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44155d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4688/4688 [==============================] - 2920s 620ms/step - loss: 0.2996 - accuracy: 0.8740 - val_loss: 0.2891 - val_accuracy: 0.8731\n",
      "Epoch 2/3\n",
      "4688/4688 [==============================] - 2903s 619ms/step - loss: 0.2427 - accuracy: 0.9012 - val_loss: 0.2600 - val_accuracy: 0.8926\n",
      "Epoch 3/3\n",
      "4688/4688 [==============================] - 2900s 619ms/step - loss: 0.2293 - accuracy: 0.9085 - val_loss: 0.2679 - val_accuracy: 0.8936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x77fa2dff4bb0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-4) \n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_inputs, train_labels, validation_data=(test_inputs, test_labels), batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2410d44",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "<1>   \n",
    "- max_length=128 : 4500sec \n",
    "- DataCollatorWithPadding : 2328.61 sec \n",
    "- Bucketing & DataCollatorWithPadding : 1804.12 sec   \n",
    "`즉 패딩에 사용되는 메모리 할당 부분을 최적화 함으로써 학습시간을 줄일 수 있다`\n",
    "\n",
    "-  Trainer 의 default lr 5e-5를 수정하여 테스트 해볼 수 있겠다. `lr:1e-5로 설정했을때 동일 에폭에서 유일하게 0.9 val acc를 넘겼다`. \n",
    "- `Freezing fine-tuning에서 학습 속도는 빨라졌지만, 성능의 변화는 미비했다.` 프루닝을 진행해볼 수 있을것 같다.\n",
    "\n",
    "의문점\n",
    "- Pytorch 기반 Trainer를 사용하였을때 사용중인 환경에서 최대 사용한 batch size는 8이였다.\n",
    "  하지만 TF로 바꿔서 학습하였을때, batch size를 16으로 늘릴 수 있었다. 하지만 학습시간은 큰 변화는 없는것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f55b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
